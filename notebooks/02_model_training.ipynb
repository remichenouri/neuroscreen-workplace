{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a2ee8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 02 - Entraînement des Modèles\\n\",\n",
    "    \"\\n\",\n",
    "    \"Développement et optimisation des modèles ML pour la détection de neurodiversité.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"import mlflow\\n\",\n",
    "    \"import mlflow.sklearn\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configuration MLflow\\n\",\n",
    "    \"mlflow.set_experiment(\\\"ubisoft_people_analytics\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"np.random.seed(42)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Chargement et préparation des données\\n\",\n",
    "    \"df = pd.read_csv('../data/sample_data.csv')\\n\",\n",
    "    \"print(f\\\"Dataset original: {df.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature engineering\\n\",\n",
    "    \"def prepare_features(data):\\n\",\n",
    "    \"    \\\"\\\"\\\"Préparer les features pour l'entraînement.\\\"\\\"\\\"\\n\",\n",
    "    \"    df_prep = data.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Encoder les variables catégorielles si elles existent\\n\",\n",
    "    \"    if 'department' in df_prep.columns:\\n\",\n",
    "    \"        le = LabelEncoder()\\n\",\n",
    "    \"        df_prep['department_encoded'] = le.fit_transform(df_prep['department'])\\n\",\n",
    "    \"        df_prep = df_prep.drop('department', axis=1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Créer des features dérivées\\n\",\n",
    "    \"    df_prep['creativity_burnout_ratio'] = df_prep['creative_score'] / (df_prep['burnout_scale'] + 1)\\n\",\n",
    "    \"    df_prep['high_creativity'] = (df_prep['creative_score'] > df_prep['creative_score'].quantile(0.75)).astype(int)\\n\",\n",
    "    \"    df_prep['high_burnout'] = (df_prep['burnout_scale'] > df_prep['burnout_scale'].quantile(0.75)).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_prep\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_features = prepare_features(df)\\n\",\n",
    "    \"print(f\\\"Après feature engineering: {df_features.shape}\\\")\\n\",\n",
    "    \"df_features.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Préparation des datasets pour TDAH et Autisme\\n\",\n",
    "    \"def prepare_ml_data(data, target_col):\\n\",\n",
    "    \"    \\\"\\\"\\\"Préparer X et y pour l'entraînement.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Exclure les colonnes non-features\\n\",\n",
    "    \"    exclude_cols = ['employee_id', 'adhd_risk', 'autism_risk'] \\n\",\n",
    "    \"    feature_cols = [col for col in data.columns if col not in exclude_cols]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    X = data[feature_cols]\\n\",\n",
    "    \"    y = data[target_col]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Features utilisées: {feature_cols}\\\")\\n\",\n",
    "    \"    print(f\\\"Distribution de {target_col}:\\\")\\n\",\n",
    "    \"    print(y.value_counts(normalize=True))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return X, y\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Préparer les données pour TDAH\\n\",\n",
    "    \"X_adhd, y_adhd = prepare_ml_data(df_features, 'adhd_risk')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split train/test\\n\",\n",
    "    \"X_train_adhd, X_test_adhd, y_train_adhd, y_test_adhd = train_test_split(\\n\",\n",
    "    \"    X_adhd, y_adhd, test_size=0.2, random_state=42, stratify=y_adhd\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nTailles des datasets TDAH:\\\")\\n\",\n",
    "    \"print(f\\\"Train: {X_train_adhd.shape}, Test: {X_test_adhd.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Définition des modèles à tester\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'RandomForest': RandomForestClassifier(random_state=42, class_weight='balanced'),\\n\",\n",
    "    \"    'GradientBoosting': GradientBoostingClassifier(random_state=42),\\n\",\n",
    "    \"    'LogisticRegression': LogisticRegression(random_state=42, class_weight='balanced'),\\n\",\n",
    "    \"    'SVM': SVC(random_state=42, class_weight='balanced', probability=True)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Grilles d'hyperparamètres\\n\",\n",
    "    \"param_grids = {\\n\",\n",
    "    \"    'RandomForest': {\\n\",\n",
    "    \"        'n_estimators': [100, 200],\\n\",\n",
    "    \"        'max_depth': [5, 10, None],\\n\",\n",
    "    \"        'min_samples_split': [2, 5],\\n\",\n",
    "    \"        'min_samples_leaf': [1, 2]\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'GradientBoosting': {\\n\",\n",
    "    \"        'n_estimators': [100, 200],\\n\",\n",
    "    \"        'learning_rate': [0.05, 0.1],\\n\",\n",
    "    \"        'max_depth': [3, 5]\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'LogisticRegression': {\\n\",\n",
    "    \"        'C': [0.1, 1, 10],\\n\",\n",
    "    \"        'penalty': ['l1', 'l2'],\\n\",\n",
    "    \"        'solver': ['liblinear']\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'SVM': {\\n\",\n",
    "    \"        'C': [0.1, 1, 10],\\n\",\n",
    "    \"        'kernel': ['rbf', 'linear'],\\n\",\n",
    "    \"        'gamma': ['scale', 'auto']\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"}\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Fonction d'entraînement et évaluation\\n\",\n",
    "    \"def train_and_evaluate_model(model, param_grid, X_train, X_test, y_train, y_test, model_name):\\n\",\n",
    "    \"    \\\"\\\"\\\"Entraîner et évaluer un modèle avec GridSearch.\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with mlflow.start_run(run_name=f\\\"{model_name}_ADHD\\\"):\\n\",\n",
    "    \"        # GridSearch avec validation croisée\\n\",\n",
    "    \"        grid_search = GridSearchCV(\\n\",\n",
    "    \"            model, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Standardisation si nécessaire\\n\",\n",
    "    \"        if model_name in ['LogisticRegression', 'SVM']:\\n\",\n",
    "    \"            scaler = StandardScaler()\\n\",\n",
    "    \"            X_train_scaled = scaler.fit_transform(X_train)\\n\",\n",
    "    \"            X_test_scaled = scaler.transform(X_test)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            X_train_scaled = X_train\\n\",\n",
    "    \"            X_test_scaled = X_test\\n\",\n",
    "    \"            scaler = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Entraînement\\n\",\n",
    "    \"        grid_search.fit(X_train_scaled, y_train)\\n\",\n",
    "    \"        best_model = grid_search.best_estimator_\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Prédictions\\n\",\n",
    "    \"        y_pred = best_model.predict(X_test_scaled)\\n\",\n",
    "    \"        y_prob = best_model.predict_proba(X_test_scaled)[:, 1]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Métriques\\n\",\n",
    "    \"        from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        accuracy = accuracy_score(y_test, y_pred)\\n\",\n",
    "    \"        precision = precision_score(y_test, y_pred)\\n\",\n",
    "    \"        recall = recall_score(y_test, y_pred)\\n\",\n",
    "    \"        f1 = f1_score(y_test, y_pred)\\n\",\n",
    "    \"        auc = roc_auc_score(y_test, y_prob)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Cross-validation score\\n\",\n",
    "    \"        cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='f1')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Log MLflow\\n\",\n",
    "    \"        mlflow.log_params(grid_search.best_params_)\\n\",\n",
    "    \"        mlflow.log_metrics({\\n\",\n",
    "    \"            'accuracy': accuracy,\\n\",\n",
    "    \"            'precision': precision,\\n\",\n",
    "    \"            'recall': recall,\\n\",\n",
    "    \"            'f1_score': f1,\\n\",\n",
    "    \"            'auc_roc': auc,\\n\",\n",
    "    \"            'cv_f1_mean': cv_scores.mean(),\\n\",\n",
    "    \"            'cv_f1_std': cv_scores.std()\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Sauvegarder le modèle\\n\",\n",
    "    \"        mlflow.sklearn.log_model(best_model, \\\"model\\\")\\n\",\n",
    "    \"        if scaler:\\n\",\n",
    "    \"            mlflow.sklearn.log_model(scaler, \\\"scaler\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Résultats\\n\",\n",
    "    \"        results = {\\n\",\n",
    "    \"            'model': best_model,\\n\",\n",
    "    \"            'scaler': scaler,\\n\",\n",
    "    \"            'best_params': grid_search.best_params_,\\n\",\n",
    "    \"            'metrics': {\\n\",\n",
    "    \"                'accuracy': accuracy,\\n\",\n",
    "    \"                'precision': precision,\\n\",\n",
    "    \"                'recall': recall,\\n\",\n",
    "    \"                'f1_score': f1,\\n\",\n",
    "    \"                'auc_roc': auc,\\n\",\n",
    "    \"                'cv_f1_mean': cv_scores.mean(),\\n\",\n",
    "    \"                'cv_f1_std': cv_scores.std()\\n\",\n",
    "    \"            },\\n\",\n",
    "    \"            'predictions': y_pred,\\n\",\n",
    "    \"            'probabilities': y_prob\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\n=== {model_name} RESULTS ===\\\")\\n\",\n",
    "    \"        print(f\\\"Best params: {grid_search.best_params_}\\\")\\n\",\n",
    "    \"        print(f\\\"F1-Score: {f1:.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"AUC-ROC: {auc:.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"CV F1: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Entraîner tous les modèles\\n\",\n",
    "    \"results = {}\\n\",\n",
    "    \"for model_name, model in models.items():\\n\",\n",
    "    \"    if model_name in param_grids:\\n\",\n",
    "    \"        results[model_name] = train_and_evaluate_model(\\n\",\n",
    "    \"            model, param_grids[model_name], \\n\",\n",
    "    \"            X_train_adhd, X_test_adhd, y_train_adhd, y_test_adhd, \\n\",\n",
    "    \"            model_name\\n\",\n",
    "    \"        )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Comparaison des modèles\\n\",\n",
    "    \"comparison_df = pd.DataFrame({\\n\",\n",
    "    \"    model_name: [\\n\",\n",
    "    \"        result['metrics']['f1_score'],\\n\",\n",
    "    \"        result['metrics']['auc_roc'],\\n\",\n",
    "    \"        result['metrics']['precision'],\\n\",\n",
    "    \"        result['metrics']['recall'],\\n\",\n",
    "    \"        result['metrics']['accuracy']\\n\",\n",
    "    \"    ] for model_name, result in results.items()\\n\",\n",
    "    \"}, index=['F1-Score', 'AUC-ROC', 'Precision', 'Recall', 'Accuracy'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== COMPARAISON DES MODÈLES ===\\\")\\n\",\n",
    "    \"print(comparison_df.round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualisation\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Heatmap des métriques\\n\",\n",
    "    \"sns.heatmap(comparison_df, annot=True, cmap='RdYlGn', ax=axes[0], cbar_kws={'shrink': 0.8})\\n\",\n",
    "    \"axes[0].set_title('Comparaison des Métriques par Modèle')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Barplot F1-Score\\n\",\n",
    "    \"f1_scores = comparison_df.loc['F1-Score'].sort_values(ascending=True)\\n\",\n",
    "    \"axes[1].barh(f1_scores.index, f1_scores.values, alpha=0.7)\\n\",\n",
    "    \"axes[1].set_title('F1-Score par Modèle')\\n\",\n",
    "    \"axes[1].set_xlabel('F1-Score')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Meilleur modèle\\n\",\n",
    "    \"best_model_name = comparison_df.loc['F1-Score'].idxmax()\\n\",\n",
    "    \"print(f\\\"\\\\n🏆 Meilleur modèle: {best_model_name} (F1-Score: {comparison_df.loc['F1-Score', best_model_name]:.3f})\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Analyse détaillée du meilleur modèle\\n\",\n",
    "    \"best_result = results[best_model_name]\\n\",\n",
    "    \"best_model = best_result['model']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Matrice de confusion\\n\",\n",
    "    \"cm = confusion_matrix(y_test_adhd, best_result['predictions'])\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"            xticklabels=['No ADHD Risk', 'ADHD Risk'],\\n\",\n",
    "    \"            yticklabels=['No ADHD Risk', 'ADHD Risk'])\\n\",\n",
    "    \"plt.title(f'Matrice de Confusion - {best_model_name}')\\n\",\n",
    "    \"plt.ylabel('Vraie Classe')\\n\",\n",
    "    \"plt.xlabel('Classe Prédite')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Courbe ROC\\n\",\n",
    "    \"fpr, tpr, thresholds = roc_curve(y_test_adhd, best_result['probabilities'])\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"plt.plot(fpr, tpr, color='darkorange', lw=2, \\n\",\n",
    "    \"         label=f'ROC Curve (AUC = {best_result[\\\"metrics\\\"][\\\"auc_roc\\\"]:.3f})')\\n\",\n",
    "    \"plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\\n\",\n",
    "    \"plt.xlim([0.0, 1.0])\\n\",\n",
    "    \"plt.ylim([0.0, 1.05])\\n\",\n",
    "    \"plt.xlabel('Taux de Faux Positifs')\\n\",\n",
    "    \"plt.ylabel('Taux de Vrais Positifs')\\n\",\n",
    "    \"plt.title(f'Courbe ROC - {best_model_name}')\\n\",\n",
    "    \"plt.legend(loc=\\\"lower right\\\")\\n\",\n",
    "    \"plt.grid(alpha=0.3)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature importance (si disponible)\\n\",\n",
    "    \"if hasattr(best_model, 'feature_importances_'):\\n\",\n",
    "    \"    feature_importance = pd.DataFrame({\\n\",\n",
    "    \"        'feature': X_adhd.columns,\\n\",\n",
    "    \"        'importance': best_model.feature_importances_\\n\",\n",
    "    \"    }).sort_values('importance', ascending=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    plt.barh(feature_importance['feature'], feature_importance['importance'], alpha=0.7)\\n\",\n",
    "    \"    plt.title(f'Importance des Features - {best_model_name}')\\n\",\n",
    "    \"    plt.xlabel('Importance')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Top 5 features les plus importantes:\\\")\\n\",\n",
    "    \"    print(feature_importance.tail().to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Sauvegarde du meilleur modèle\\n\",\n",
    "    \"model_dir = Path('../models')\\n\",\n",
    "    \"model_dir.mkdir(exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sauvegarder le modèle\\n\",\n",
    "    \"joblib.dump(best_model, model_dir / 'random_forest.pkl')\\n\",\n",
    "    \"if best_result['scaler']:\\n\",\n",
    "    \"    joblib.dump(best_result['scaler'], model_dir / 'scaler.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sauvegarder les métadonnées\\n\",\n",
    "    \"metadata = {\\n\",\n",
    "    \"    'model_name': best_model_name,\\n\",\n",
    "    \"    'model_type': 'ADHD_classifier',\\n\",\n",
    "    \"    'training_date': pd.Timestamp.now().isoformat(),\\n\",\n",
    "    \"    'feature_names': X_adhd.columns.tolist(),\\n\",\n",
    "    \"    'best_params': best_result['best_params'],\\n\",\n",
    "    \"    'metrics': best_result['metrics'],\\n\",\n",
    "    \"    'target_variable': 'adhd_risk'\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"with open(model_dir / 'model_metadata.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(metadata, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"✅ Modèle sauvegardé: {model_dir / 'random_forest.pkl'}\\\")\\n\",\n",
    "    \"print(f\\\"✅ Métadonnées sauvegardées: {model_dir / 'model_metadata.json'}\\\")\\n\",\n",
    "    \"print(f\\\"✅ Performances finales: F1={best_result['metrics']['f1_score']:.3f}, AUC={best_result['metrics']['auc_roc']:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Test du modèle sauvegardé\\n\",\n",
    "    \"print(\\\"=== TEST DU MODÈLE SAUVEGARDÉ ===\\\")\\n\",\n",
    "    \"loaded_model = joblib.load(model_dir / 'random_forest.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test sur un échantillon\\n\",\n",
    "    \"test_sample = X_test_adhd.iloc[:5]\\n\",\n",
    "    \"predictions = loaded_model.predict(test_sample)\\n\",\n",
    "    \"probabilities = loaded_model.predict_proba(test_sample)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Prédictions sur échantillon:\\\")\\n\",\n",
    "    \"for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\\n\",\n",
    "    \"    print(f\\\"Employé {i+1}: Prédiction={pred}, Probabilité TDAH={prob[1]:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n=== RÉSUMÉ FINAL ===\\\")\\n\",\n",
    "    \"print(f\\\"• Meilleur modèle: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"• F1-Score: {best_result['metrics']['f1_score']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"• Précision: {best_result['metrics']['precision']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"• Rappel: {best_result['metrics']['recall']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"• AUC-ROC: {best_result['metrics']['auc_roc']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"• Validation croisée: {best_result['metrics']['cv_f1_mean']:.3f} ± {best_result['metrics']['cv_f1_std']:.3f}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\\n   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
